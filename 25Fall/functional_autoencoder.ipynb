{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd5a8f1",
   "metadata": {},
   "source": [
    "# Functional Autoencoder(2025Fall Seminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f1afda",
   "metadata": {},
   "source": [
    "### Functional Autoencoder realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4031edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.interpolate import BSpline, splrep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class FunctionalAutoencoder(nn.Module):\n",
    "    def __init__(self, t_grid, num_basis=20, hidden_dim=32, hidden_dim2=16, latent_dim=8, degree=3):\n",
    "        super(FunctionalAutoencoder, self).__init__()\n",
    "        self.t_grid = t_grid         \n",
    "        self.M = len(t_grid)\n",
    "        self.dt = t_grid[1] - t_grid[0]  # assume uniform grid\n",
    "        self.num_basis = num_basis\n",
    "        self.degree = degree\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        # B-spline basis (uniform, clamped at the endpoints)\n",
    "        self.knots = self._make_clamped_uniform_knots(num_basis=self.num_basis, degree=self.degree)\n",
    "        basis_matrix = self._compute_basis_matrix()  # shape (num_basis, M)\n",
    "        self.register_buffer('basis_matrix', basis_matrix)\n",
    "\n",
    "        # Encoder functional weights: each hidden unit has num_basis coefficients\n",
    "        # w_k(t) = sum_l c_kl * B_l(t)\n",
    "        self.encoder_coeffs = nn.Parameter(torch.randn(hidden_dim, num_basis) * 0.01)\n",
    "\n",
    "        # Encoder fully-connected layers: integrals -> hidden_dim -> hidden_dim2 -> latent_dim\n",
    "        self.fc_encoder_1 = nn.Linear(hidden_dim, hidden_dim2)\n",
    "        self.fc_encoder_2 = nn.Linear(hidden_dim2, latent_dim)\n",
    "\n",
    "        # Decoder fully-connected layers: latent_dim -> hidden_dim2 -> hidden_dim\n",
    "        self.fc_decoder_1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.fc_decoder_2 = nn.Linear(hidden_dim2, hidden_dim)\n",
    "\n",
    "        # Decoder functional weights: each hidden unit has num_basis coefficients\n",
    "        # v_k(t) = sum_l d_kl * B_l(t)\n",
    "        self.decoder_coeffs = nn.Parameter(torch.randn(hidden_dim, num_basis) * 0.01)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def _make_clamped_uniform_knots(self, num_basis, degree):\n",
    "        n = num_basis\n",
    "        k = degree\n",
    "        if n <= k:\n",
    "            raise ValueError(\"num_basis must be greater than degree\")\n",
    "        num_internal = n - k - 1\n",
    "        if num_internal > 0:\n",
    "            internal = np.linspace(0.0, 1.0, num_internal + 2)[1:-1]\n",
    "        else:\n",
    "            internal = np.array([])\n",
    "        t = np.concatenate([\n",
    "            np.zeros(k + 1),\n",
    "            internal,\n",
    "            np.ones(k + 1)\n",
    "        ])\n",
    "        return t\n",
    "\n",
    "    def _compute_basis_matrix(self):\n",
    "        \"\"\"Evaluate B-spline basis functions on the grid, shape (num_basis, M).\"\"\"\n",
    "        basis_matrix = np.zeros((self.num_basis, self.M))\n",
    "        eye = np.eye(self.num_basis)\n",
    "        for l in range(self.num_basis):\n",
    "            spline = BSpline(self.knots, eye[l], k=self.degree, extrapolate=True)\n",
    "            basis_matrix[l] = spline(self.t_grid)\n",
    "        return torch.tensor(basis_matrix, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: input functional data, shape (batch_size, M).\"\"\"\n",
    "        # Encoder: functional inner products -> FC layers -> latent\n",
    "        w_grid = self.encoder_coeffs @ self.basis_matrix        # (hidden_dim, M)\n",
    "        integrals = torch.einsum('bm,km->bk', x, w_grid) * self.dt\n",
    "\n",
    "        hidden1 = self.activation(integrals)\n",
    "        hidden2 = self.activation(self.fc_encoder_1(hidden1))\n",
    "        latent = self.fc_encoder_2(hidden2)\n",
    "\n",
    "        # Decoder: latent -> FC layers -> functional reconstruction\n",
    "        hidden2_dec = self.activation(self.fc_decoder_1(latent))\n",
    "        hidden1_dec = self.activation(self.fc_decoder_2(hidden2_dec))\n",
    "\n",
    "        v_grid = self.decoder_coeffs @ self.basis_matrix        # (hidden_dim, M)\n",
    "        recon = torch.einsum('bk,km->bm', hidden1_dec, v_grid)\n",
    "\n",
    "        return recon, latent\n",
    "\n",
    "\n",
    "def train_fae(\n",
    "    data,\n",
    "    t_grid,\n",
    "    epochs=500,\n",
    "    lr=0.005,\n",
    "    batch_size=32,\n",
    "    num_basis=20,\n",
    "    hidden_dim=32,\n",
    "    hidden_dim2=16,\n",
    "    latent_dim=8,\n",
    "    degree=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the functional autoencoder on discretized functional data.\n",
    "\n",
    "    data: numpy array of shape (n_samples, M)\n",
    "    t_grid: numpy array of shape (M,)\n",
    "    \"\"\"\n",
    "    model = FunctionalAutoencoder(\n",
    "        t_grid,\n",
    "        num_basis=num_basis,\n",
    "        hidden_dim=hidden_dim,\n",
    "        hidden_dim2=hidden_dim2,\n",
    "        latent_dim=latent_dim,\n",
    "        degree=degree\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(data, dtype=torch.float32))\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(\"\\n--- Training FAE ---\")\n",
    "    final_loss = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in loader:\n",
    "            x = batch[0] \n",
    "            recon, _ = model(x)\n",
    "            loss = criterion(recon, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader.dataset)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "        if epoch == epochs - 1:\n",
    "            final_loss = avg_loss\n",
    "\n",
    "    return model, final_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f15e35",
   "metadata": {},
   "source": [
    "#### FPCA realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18adabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skfda.representation.grid import FDataGrid\n",
    "from skfda.preprocessing.dim_reduction import FPCA\n",
    "\n",
    "def train_fpca_skfda(data, t_grid, n_components):\n",
    "    fd = FDataGrid(data_matrix=data, grid_points=t_grid)\n",
    "    fpca = FPCA(n_components=n_components)\n",
    "    fpca.fit(fd)\n",
    "    scores = fpca.transform(fd)\n",
    "    fd_recon = fpca.inverse_transform(scores)\n",
    "    recon = fd_recon.data_matrix[:, 0, :]\n",
    "    mse = np.mean((recon - data) ** 2)\n",
    "    return fpca, scores, recon, mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938918f1",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf3aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_case(case: int, n: int = 100, T: int = 51, delta: float = 0.1, seed: int = None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Observation grid: 51 equally spaced points on [0,1]\n",
    "    t_grid = np.linspace(0.0, 1.0, T)\n",
    "\n",
    "    X = np.zeros((n, T))\n",
    "    Y = np.zeros((n, T))\n",
    "\n",
    "    for i in range(n):\n",
    "        # Generate ξ_{i1}, ξ_{i2} based on different cases\n",
    "        if case == 1:\n",
    "            # ξ_{i1} ~ N(0, 3^2), ξ_{i2} ~ N(0, 2^2)\n",
    "            xi1 = np.random.normal(0.0, 3.0)\n",
    "            xi2 = np.random.normal(0.0, 2.0)\n",
    "            Xi = xi1 * np.sin(2 * np.pi * t_grid) + xi2 * np.cos(2 * np.pi * t_grid)\n",
    "\n",
    "        elif case == 2:\n",
    "            # ξ_{i1}, ξ_{i2} ~ N(0, 2^2)\n",
    "            xi1 = np.random.normal(0.0, 2.0)\n",
    "            xi2 = np.random.normal(0.0, 2.0)\n",
    "            Xi = xi2 * np.sin(xi1 * t_grid)\n",
    "\n",
    "        elif case == 3:\n",
    "            # Same distribution as Case 2\n",
    "            xi1 = np.random.normal(0.0, 2.0)\n",
    "            xi2 = np.random.normal(0.0, 2.0)\n",
    "            Xi = xi2 * np.cos(xi1 * t_grid)\n",
    "\n",
    "        elif case == 4:\n",
    "            # ξ_{i1}, ξ_{i2} ~ N(0, 2^2)\n",
    "            xi1 = np.random.normal(0.0, 2.0)\n",
    "            xi2 = np.random.normal(0.0, 2.0)\n",
    "            Xi = (\n",
    "                xi1 * np.sin(2 * np.pi * t_grid)\n",
    "                + xi2 * np.cos(2 * np.pi * t_grid)\n",
    "                + xi2 * np.sin(xi1 * t_grid)\n",
    "            )\n",
    "\n",
    "        elif case == 5:\n",
    "            # ξ_{i1}, ξ_{i2} ~ N(0, 2^2)\n",
    "            xi1 = np.random.normal(0.0, 2.0)\n",
    "            xi2 = np.random.normal(0.0, 2.0)\n",
    "            Xi = (\n",
    "                xi1 * np.sin(2 * np.pi * t_grid)\n",
    "                + xi2 * np.cos(2 * np.pi * t_grid)\n",
    "                + xi2 * np.cos(xi1 * t_grid)\n",
    "            )\n",
    "\n",
    "        # Save true curve\n",
    "        X[i, :] = Xi\n",
    "\n",
    "        # Add measurement error ε_ij ~ N(0, δ^2)\n",
    "        eps = np.random.normal(0.0, delta, size=T)\n",
    "        Y[i, :] = Xi + eps\n",
    "\n",
    "    return t_grid, X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d2807",
   "metadata": {},
   "source": [
    "#### Reconstruction Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5b7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Comparing FAE and FPCA Reconstruction Loss\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Case 1\n",
      "============================================================\n",
      "\n",
      "--- Run 1/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 225.0836\n",
      "Epoch 20, Loss: 162.3332\n",
      "Epoch 30, Loss: 96.2664\n",
      "Epoch 40, Loss: 25.9556\n",
      "Epoch 50, Loss: 2.6854\n",
      "Epoch 60, Loss: 8.5345\n",
      "Epoch 70, Loss: 0.9257\n",
      "Epoch 80, Loss: 0.7692\n",
      "Epoch 90, Loss: 0.7001\n",
      "Epoch 100, Loss: 0.6806\n",
      "Epoch 110, Loss: 1.3357\n",
      "Epoch 120, Loss: 0.7680\n",
      "Epoch 130, Loss: 1.1014\n",
      "Epoch 140, Loss: 0.6171\n",
      "Epoch 150, Loss: 0.5930\n",
      "Epoch 160, Loss: 0.9308\n",
      "Epoch 170, Loss: 2.3322\n",
      "Epoch 180, Loss: 0.7030\n",
      "Epoch 190, Loss: 0.6343\n",
      "Epoch 200, Loss: 0.5561\n",
      "Epoch 210, Loss: 0.6183\n",
      "Epoch 220, Loss: 3.9192\n",
      "Epoch 230, Loss: 0.6008\n",
      "Epoch 240, Loss: 0.6575\n",
      "Epoch 250, Loss: 1.1893\n",
      "Epoch 260, Loss: 0.6348\n",
      "Epoch 270, Loss: 0.5211\n",
      "Epoch 280, Loss: 0.5200\n",
      "Epoch 290, Loss: 0.7235\n",
      "Epoch 300, Loss: 2.1958\n",
      "FAE Loss: 2.195778\n",
      "Training FPCA...\n",
      "FPCA Loss: 10.327919\n",
      "\n",
      "--- Run 2/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 295.9275\n",
      "Epoch 20, Loss: 29.2329\n",
      "Epoch 30, Loss: 2.3194\n",
      "Epoch 40, Loss: 1.1638\n",
      "Epoch 50, Loss: 1.3919\n",
      "Epoch 60, Loss: 0.7336\n",
      "Epoch 70, Loss: 0.6034\n",
      "Epoch 80, Loss: 0.5779\n",
      "Epoch 90, Loss: 0.6807\n",
      "Epoch 100, Loss: 1.4726\n",
      "Epoch 110, Loss: 0.5840\n",
      "Epoch 120, Loss: 0.5546\n",
      "Epoch 130, Loss: 0.5580\n",
      "Epoch 140, Loss: 0.8085\n",
      "Epoch 150, Loss: 1.2280\n",
      "Epoch 160, Loss: 0.5257\n",
      "Epoch 170, Loss: 0.5195\n",
      "Epoch 180, Loss: 0.5219\n",
      "Epoch 190, Loss: 0.5726\n",
      "Epoch 200, Loss: 4.0359\n",
      "Epoch 210, Loss: 0.6453\n",
      "Epoch 220, Loss: 0.7251\n",
      "Epoch 230, Loss: 0.9519\n",
      "Epoch 240, Loss: 0.6559\n",
      "Epoch 250, Loss: 0.5214\n",
      "Epoch 260, Loss: 0.5727\n",
      "Epoch 270, Loss: 1.3968\n",
      "Epoch 280, Loss: 1.0104\n",
      "Epoch 290, Loss: 0.6962\n",
      "Epoch 300, Loss: 1.2733\n",
      "FAE Loss: 1.273281\n",
      "Training FPCA...\n",
      "FPCA Loss: 10.723309\n",
      "\n",
      "--- Run 3/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 160.6099\n",
      "Epoch 20, Loss: 91.6318\n",
      "Epoch 30, Loss: 90.6163\n",
      "Epoch 40, Loss: 89.9325\n",
      "Epoch 50, Loss: 89.5824\n",
      "Epoch 60, Loss: 89.9625\n",
      "Epoch 70, Loss: 90.1228\n",
      "Epoch 80, Loss: 88.1097\n",
      "Epoch 90, Loss: 60.8895\n",
      "Epoch 100, Loss: 50.2210\n",
      "Epoch 110, Loss: 7.1747\n",
      "Epoch 120, Loss: 3.9139\n",
      "Epoch 130, Loss: 0.8342\n",
      "Epoch 140, Loss: 0.5847\n",
      "Epoch 150, Loss: 0.5510\n",
      "Epoch 160, Loss: 0.5141\n",
      "Epoch 170, Loss: 0.4987\n",
      "Epoch 180, Loss: 0.4927\n",
      "Epoch 190, Loss: 0.4863\n",
      "Epoch 200, Loss: 0.4813\n",
      "Epoch 210, Loss: 0.4848\n",
      "Epoch 220, Loss: 0.4770\n",
      "Epoch 230, Loss: 0.5088\n",
      "Epoch 240, Loss: 1.2619\n",
      "Epoch 250, Loss: 0.6953\n",
      "Epoch 260, Loss: 0.4997\n",
      "Epoch 270, Loss: 0.5024\n",
      "Epoch 280, Loss: 1.1967\n",
      "Epoch 290, Loss: 0.7914\n",
      "Epoch 300, Loss: 0.8038\n",
      "FAE Loss: 0.803829\n",
      "Training FPCA...\n",
      "FPCA Loss: 9.501265\n",
      "\n",
      "--- Run 4/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 189.8299\n",
      "Epoch 20, Loss: 99.4849\n",
      "Epoch 30, Loss: 94.0468\n",
      "Epoch 40, Loss: 92.9667\n",
      "Epoch 50, Loss: 92.1216\n",
      "Epoch 60, Loss: 90.7191\n",
      "Epoch 70, Loss: 41.5928\n",
      "Epoch 80, Loss: 17.8641\n",
      "Epoch 90, Loss: 4.4803\n",
      "Epoch 100, Loss: 1.3442\n",
      "Epoch 110, Loss: 0.9844\n",
      "Epoch 120, Loss: 0.9488\n",
      "Epoch 130, Loss: 0.8218\n",
      "Epoch 140, Loss: 0.7862\n",
      "Epoch 150, Loss: 0.8526\n",
      "Epoch 160, Loss: 0.6468\n",
      "Epoch 170, Loss: 0.5887\n",
      "Epoch 180, Loss: 0.5711\n",
      "Epoch 190, Loss: 0.5760\n",
      "Epoch 200, Loss: 0.7072\n",
      "Epoch 210, Loss: 1.9352\n",
      "Epoch 220, Loss: 0.6439\n",
      "Epoch 230, Loss: 0.5672\n",
      "Epoch 240, Loss: 0.5445\n",
      "Epoch 250, Loss: 0.5407\n",
      "Epoch 260, Loss: 0.5592\n",
      "Epoch 270, Loss: 0.8248\n",
      "Epoch 280, Loss: 0.8350\n",
      "Epoch 290, Loss: 0.9302\n",
      "Epoch 300, Loss: 2.0357\n",
      "FAE Loss: 2.035665\n",
      "Training FPCA...\n",
      "FPCA Loss: 9.779437\n",
      "\n",
      "--- Run 5/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 211.7157\n",
      "Epoch 20, Loss: 97.3333\n",
      "Epoch 30, Loss: 95.3230\n",
      "Epoch 40, Loss: 94.5434\n",
      "Epoch 50, Loss: 94.0025\n",
      "Epoch 60, Loss: 92.5598\n",
      "Epoch 70, Loss: 75.2037\n",
      "Epoch 80, Loss: 11.2719\n",
      "Epoch 90, Loss: 3.7325\n",
      "Epoch 100, Loss: 2.1861\n",
      "Epoch 110, Loss: 2.9346\n",
      "Epoch 120, Loss: 1.4501\n",
      "Epoch 130, Loss: 0.7853\n",
      "Epoch 140, Loss: 0.6743\n",
      "Epoch 150, Loss: 0.6348\n",
      "Epoch 160, Loss: 0.7108\n",
      "Epoch 170, Loss: 2.9286\n",
      "Epoch 180, Loss: 0.8397\n",
      "Epoch 190, Loss: 0.5925\n",
      "Epoch 200, Loss: 0.8643\n",
      "Epoch 210, Loss: 1.8795\n",
      "Epoch 220, Loss: 0.6562\n",
      "Epoch 230, Loss: 1.1680\n",
      "Epoch 240, Loss: 0.8032\n",
      "Epoch 250, Loss: 1.2579\n",
      "Epoch 260, Loss: 2.1937\n",
      "Epoch 270, Loss: 0.6463\n",
      "Epoch 280, Loss: 0.7913\n",
      "Epoch 290, Loss: 0.9355\n",
      "Epoch 300, Loss: 0.5775\n",
      "FAE Loss: 0.577472\n",
      "Training FPCA...\n",
      "FPCA Loss: 9.703989\n",
      "\n",
      "============================================================\n",
      "Case 1 Summary:\n",
      "FAE:  1.377205 ± 0.645409\n",
      "FPCA: 10.007184 ± 0.450656\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Case 2\n",
      "============================================================\n",
      "\n",
      "--- Run 1/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 34.3522\n",
      "Epoch 20, Loss: 10.4171\n",
      "Epoch 30, Loss: 8.8175\n",
      "Epoch 40, Loss: 8.8300\n",
      "Epoch 50, Loss: 8.7457\n",
      "Epoch 60, Loss: 8.7813\n",
      "Epoch 70, Loss: 8.8162\n",
      "Epoch 80, Loss: 8.7776\n",
      "Epoch 90, Loss: 8.7553\n",
      "Epoch 100, Loss: 8.7725\n",
      "Epoch 110, Loss: 8.9492\n",
      "Epoch 120, Loss: 8.7531\n",
      "Epoch 130, Loss: 8.7325\n",
      "Epoch 140, Loss: 8.7949\n",
      "Epoch 150, Loss: 8.7613\n",
      "Epoch 160, Loss: 8.8579\n",
      "Epoch 170, Loss: 8.7398\n",
      "Epoch 180, Loss: 8.8439\n",
      "Epoch 190, Loss: 8.7706\n",
      "Epoch 200, Loss: 8.7858\n",
      "Epoch 210, Loss: 8.7167\n",
      "Epoch 220, Loss: 8.7217\n",
      "Epoch 230, Loss: 8.7960\n",
      "Epoch 240, Loss: 9.0332\n",
      "Epoch 250, Loss: 8.4171\n",
      "Epoch 260, Loss: 2.6711\n",
      "Epoch 270, Loss: 2.1692\n",
      "Epoch 280, Loss: 2.0153\n",
      "Epoch 290, Loss: 2.1379\n",
      "Epoch 300, Loss: 2.0963\n",
      "FAE Loss: 2.096284\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.382529\n",
      "\n",
      "--- Run 2/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 43.6875\n",
      "Epoch 20, Loss: 11.2217\n",
      "Epoch 30, Loss: 10.1494\n",
      "Epoch 40, Loss: 10.0899\n",
      "Epoch 50, Loss: 9.9125\n",
      "Epoch 60, Loss: 9.6784\n",
      "Epoch 70, Loss: 6.7189\n",
      "Epoch 80, Loss: 2.6943\n",
      "Epoch 90, Loss: 1.6747\n",
      "Epoch 100, Loss: 1.5915\n",
      "Epoch 110, Loss: 1.5334\n",
      "Epoch 120, Loss: 1.4788\n",
      "Epoch 130, Loss: 1.4656\n",
      "Epoch 140, Loss: 1.4522\n",
      "Epoch 150, Loss: 1.4507\n",
      "Epoch 160, Loss: 1.4315\n",
      "Epoch 170, Loss: 1.4246\n",
      "Epoch 180, Loss: 1.4586\n",
      "Epoch 190, Loss: 1.4812\n",
      "Epoch 200, Loss: 1.3778\n",
      "Epoch 210, Loss: 1.3532\n",
      "Epoch 220, Loss: 1.1593\n",
      "Epoch 230, Loss: 1.0793\n",
      "Epoch 240, Loss: 0.5906\n",
      "Epoch 250, Loss: 0.5177\n",
      "Epoch 260, Loss: 0.5111\n",
      "Epoch 270, Loss: 0.5058\n",
      "Epoch 280, Loss: 0.5337\n",
      "Epoch 290, Loss: 0.5673\n",
      "Epoch 300, Loss: 0.5226\n",
      "FAE Loss: 0.522591\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.177894\n",
      "\n",
      "--- Run 3/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 51.7592\n",
      "Epoch 20, Loss: 7.3824\n",
      "Epoch 30, Loss: 4.6974\n",
      "Epoch 40, Loss: 4.5886\n",
      "Epoch 50, Loss: 4.5608\n",
      "Epoch 60, Loss: 4.5426\n",
      "Epoch 70, Loss: 4.4821\n",
      "Epoch 80, Loss: 4.4290\n",
      "Epoch 90, Loss: 4.4784\n",
      "Epoch 100, Loss: 4.4558\n",
      "Epoch 110, Loss: 4.4225\n",
      "Epoch 120, Loss: 4.4055\n",
      "Epoch 130, Loss: 4.3088\n",
      "Epoch 140, Loss: 4.3516\n",
      "Epoch 150, Loss: 4.3694\n",
      "Epoch 160, Loss: 4.3744\n",
      "Epoch 170, Loss: 4.4349\n",
      "Epoch 180, Loss: 4.2565\n",
      "Epoch 190, Loss: 4.2464\n",
      "Epoch 200, Loss: 4.1919\n",
      "Epoch 210, Loss: 4.2326\n",
      "Epoch 220, Loss: 4.2128\n",
      "Epoch 230, Loss: 4.2083\n",
      "Epoch 240, Loss: 4.2010\n",
      "Epoch 250, Loss: 4.3380\n",
      "Epoch 260, Loss: 4.2348\n",
      "Epoch 270, Loss: 4.1175\n",
      "Epoch 280, Loss: 4.1517\n",
      "Epoch 290, Loss: 4.0789\n",
      "Epoch 300, Loss: 4.0929\n",
      "FAE Loss: 4.092898\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.586069\n",
      "\n",
      "--- Run 4/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 57.8946\n",
      "Epoch 20, Loss: 13.7761\n",
      "Epoch 30, Loss: 11.9784\n",
      "Epoch 40, Loss: 11.6926\n",
      "Epoch 50, Loss: 11.5802\n",
      "Epoch 60, Loss: 11.7102\n",
      "Epoch 70, Loss: 11.5592\n",
      "Epoch 80, Loss: 11.5259\n",
      "Epoch 90, Loss: 11.6113\n",
      "Epoch 100, Loss: 11.5145\n",
      "Epoch 110, Loss: 12.0586\n",
      "Epoch 120, Loss: 11.4439\n",
      "Epoch 130, Loss: 10.7856\n",
      "Epoch 140, Loss: 5.9489\n",
      "Epoch 150, Loss: 5.7642\n",
      "Epoch 160, Loss: 1.7639\n",
      "Epoch 170, Loss: 1.5780\n",
      "Epoch 180, Loss: 1.3618\n",
      "Epoch 190, Loss: 1.1334\n",
      "Epoch 200, Loss: 0.9365\n",
      "Epoch 210, Loss: 0.7537\n",
      "Epoch 220, Loss: 0.6802\n",
      "Epoch 230, Loss: 1.5504\n",
      "Epoch 240, Loss: 0.5485\n",
      "Epoch 250, Loss: 0.5084\n",
      "Epoch 260, Loss: 0.4930\n",
      "Epoch 270, Loss: 0.4907\n",
      "Epoch 280, Loss: 0.4973\n",
      "Epoch 290, Loss: 0.5523\n",
      "Epoch 300, Loss: 0.9473\n",
      "FAE Loss: 0.947302\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.921914\n",
      "\n",
      "--- Run 5/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 20.1016\n",
      "Epoch 20, Loss: 7.0214\n",
      "Epoch 30, Loss: 6.3425\n",
      "Epoch 40, Loss: 6.3129\n",
      "Epoch 50, Loss: 6.2231\n",
      "Epoch 60, Loss: 6.2194\n",
      "Epoch 70, Loss: 6.2079\n",
      "Epoch 80, Loss: 6.1571\n",
      "Epoch 90, Loss: 6.0720\n",
      "Epoch 100, Loss: 6.0624\n",
      "Epoch 110, Loss: 6.0693\n",
      "Epoch 120, Loss: 5.6773\n",
      "Epoch 130, Loss: 4.9266\n",
      "Epoch 140, Loss: 3.1306\n",
      "Epoch 150, Loss: 3.6130\n",
      "Epoch 160, Loss: 2.6910\n",
      "Epoch 170, Loss: 2.5107\n",
      "Epoch 180, Loss: 2.0904\n",
      "Epoch 190, Loss: 1.1517\n",
      "Epoch 200, Loss: 1.0132\n",
      "Epoch 210, Loss: 0.9828\n",
      "Epoch 220, Loss: 2.5065\n",
      "Epoch 230, Loss: 0.9640\n",
      "Epoch 240, Loss: 0.8935\n",
      "Epoch 250, Loss: 0.7590\n",
      "Epoch 260, Loss: 0.6670\n",
      "Epoch 270, Loss: 0.6073\n",
      "Epoch 280, Loss: 0.5648\n",
      "Epoch 290, Loss: 0.5676\n",
      "Epoch 300, Loss: 1.0370\n",
      "FAE Loss: 1.036977\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.112919\n",
      "\n",
      "============================================================\n",
      "Case 2 Summary:\n",
      "FAE:  1.739210 ± 1.285905\n",
      "FPCA: 1.436265 ± 0.294106\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Case 3\n",
      "============================================================\n",
      "\n",
      "--- Run 1/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 76.5732\n",
      "Epoch 20, Loss: 14.9056\n",
      "Epoch 30, Loss: 13.4890\n",
      "Epoch 40, Loss: 12.9832\n",
      "Epoch 50, Loss: 12.4509\n",
      "Epoch 60, Loss: 11.7021\n",
      "Epoch 70, Loss: 11.1736\n",
      "Epoch 80, Loss: 10.4810\n",
      "Epoch 90, Loss: 10.1980\n",
      "Epoch 100, Loss: 8.3334\n",
      "Epoch 110, Loss: 6.6912\n",
      "Epoch 120, Loss: 4.9340\n",
      "Epoch 130, Loss: 3.3895\n",
      "Epoch 140, Loss: 3.3835\n",
      "Epoch 150, Loss: 0.9255\n",
      "Epoch 160, Loss: 0.8134\n",
      "Epoch 170, Loss: 0.9388\n",
      "Epoch 180, Loss: 1.2210\n",
      "Epoch 190, Loss: 0.7225\n",
      "Epoch 200, Loss: 0.6331\n",
      "Epoch 210, Loss: 0.6137\n",
      "Epoch 220, Loss: 0.6588\n",
      "Epoch 230, Loss: 1.3536\n",
      "Epoch 240, Loss: 0.5657\n",
      "Epoch 250, Loss: 0.5430\n",
      "Epoch 260, Loss: 0.5271\n",
      "Epoch 270, Loss: 0.6180\n",
      "Epoch 280, Loss: 2.0589\n",
      "Epoch 290, Loss: 0.5460\n",
      "Epoch 300, Loss: 0.5709\n",
      "FAE Loss: 0.570943\n",
      "Training FPCA...\n",
      "FPCA Loss: 0.814574\n",
      "\n",
      "--- Run 2/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 86.8618\n",
      "Epoch 20, Loss: 29.0403\n",
      "Epoch 30, Loss: 27.2238\n",
      "Epoch 40, Loss: 26.0256\n",
      "Epoch 50, Loss: 12.8472\n",
      "Epoch 60, Loss: 9.9121\n",
      "Epoch 70, Loss: 5.9031\n",
      "Epoch 80, Loss: 5.2092\n",
      "Epoch 90, Loss: 4.8134\n",
      "Epoch 100, Loss: 1.7863\n",
      "Epoch 110, Loss: 1.2856\n",
      "Epoch 120, Loss: 1.2410\n",
      "Epoch 130, Loss: 1.4001\n",
      "Epoch 140, Loss: 1.5227\n",
      "Epoch 150, Loss: 1.2468\n",
      "Epoch 160, Loss: 1.2379\n",
      "Epoch 170, Loss: 1.2881\n",
      "Epoch 180, Loss: 1.9352\n",
      "Epoch 190, Loss: 1.2180\n",
      "Epoch 200, Loss: 1.1486\n",
      "Epoch 210, Loss: 1.2654\n",
      "Epoch 220, Loss: 1.1429\n",
      "Epoch 230, Loss: 1.2722\n",
      "Epoch 240, Loss: 2.3914\n",
      "Epoch 250, Loss: 1.1902\n",
      "Epoch 260, Loss: 1.3306\n",
      "Epoch 270, Loss: 1.1420\n",
      "Epoch 280, Loss: 1.1219\n",
      "Epoch 290, Loss: 1.2752\n",
      "Epoch 300, Loss: 1.4861\n",
      "FAE Loss: 1.486104\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.728843\n",
      "\n",
      "--- Run 3/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 95.9446\n",
      "Epoch 20, Loss: 35.9253\n",
      "Epoch 30, Loss: 32.8348\n",
      "Epoch 40, Loss: 28.4317\n",
      "Epoch 50, Loss: 26.9713\n",
      "Epoch 60, Loss: 26.0203\n",
      "Epoch 70, Loss: 23.7020\n",
      "Epoch 80, Loss: 18.3076\n",
      "Epoch 90, Loss: 15.6232\n",
      "Epoch 100, Loss: 14.5951\n",
      "Epoch 110, Loss: 8.5368\n",
      "Epoch 120, Loss: 6.0363\n",
      "Epoch 130, Loss: 4.3130\n",
      "Epoch 140, Loss: 5.6889\n",
      "Epoch 150, Loss: 3.6232\n",
      "Epoch 160, Loss: 3.4055\n",
      "Epoch 170, Loss: 3.3196\n",
      "Epoch 180, Loss: 3.1741\n",
      "Epoch 190, Loss: 2.8694\n",
      "Epoch 200, Loss: 2.9199\n",
      "Epoch 210, Loss: 3.1649\n",
      "Epoch 220, Loss: 2.8131\n",
      "Epoch 230, Loss: 2.5206\n",
      "Epoch 240, Loss: 2.2743\n",
      "Epoch 250, Loss: 2.2589\n",
      "Epoch 260, Loss: 2.6776\n",
      "Epoch 270, Loss: 2.2402\n",
      "Epoch 280, Loss: 2.2717\n",
      "Epoch 290, Loss: 2.0735\n",
      "Epoch 300, Loss: 2.0680\n",
      "FAE Loss: 2.067990\n",
      "Training FPCA...\n",
      "FPCA Loss: 2.220335\n",
      "\n",
      "--- Run 4/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 87.7507\n",
      "Epoch 20, Loss: 25.1285\n",
      "Epoch 30, Loss: 23.4000\n",
      "Epoch 40, Loss: 22.9792\n",
      "Epoch 50, Loss: 22.4733\n",
      "Epoch 60, Loss: 21.8418\n",
      "Epoch 70, Loss: 21.5603\n",
      "Epoch 80, Loss: 21.1253\n",
      "Epoch 90, Loss: 20.7269\n",
      "Epoch 100, Loss: 20.3586\n",
      "Epoch 110, Loss: 16.2509\n",
      "Epoch 120, Loss: 7.8136\n",
      "Epoch 130, Loss: 4.5124\n",
      "Epoch 140, Loss: 2.4927\n",
      "Epoch 150, Loss: 1.6532\n",
      "Epoch 160, Loss: 1.1722\n",
      "Epoch 170, Loss: 5.4797\n",
      "Epoch 180, Loss: 1.1540\n",
      "Epoch 190, Loss: 0.9689\n",
      "Epoch 200, Loss: 0.8165\n",
      "Epoch 210, Loss: 0.9076\n",
      "Epoch 220, Loss: 0.7414\n",
      "Epoch 230, Loss: 0.8566\n",
      "Epoch 240, Loss: 0.9516\n",
      "Epoch 250, Loss: 0.7609\n",
      "Epoch 260, Loss: 0.7012\n",
      "Epoch 270, Loss: 0.6593\n",
      "Epoch 280, Loss: 0.6488\n",
      "Epoch 290, Loss: 0.6754\n",
      "Epoch 300, Loss: 1.3638\n",
      "FAE Loss: 1.363842\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.474016\n",
      "\n",
      "--- Run 5/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 69.6240\n",
      "Epoch 20, Loss: 15.7567\n",
      "Epoch 30, Loss: 14.6357\n",
      "Epoch 40, Loss: 14.0115\n",
      "Epoch 50, Loss: 14.3967\n",
      "Epoch 60, Loss: 9.5579\n",
      "Epoch 70, Loss: 6.4801\n",
      "Epoch 80, Loss: 5.3528\n",
      "Epoch 90, Loss: 5.3743\n",
      "Epoch 100, Loss: 5.0540\n",
      "Epoch 110, Loss: 4.9997\n",
      "Epoch 120, Loss: 2.0645\n",
      "Epoch 130, Loss: 1.6616\n",
      "Epoch 140, Loss: 0.7820\n",
      "Epoch 150, Loss: 0.6246\n",
      "Epoch 160, Loss: 0.7287\n",
      "Epoch 170, Loss: 0.5402\n",
      "Epoch 180, Loss: 0.5372\n",
      "Epoch 190, Loss: 0.8320\n",
      "Epoch 200, Loss: 0.8158\n",
      "Epoch 210, Loss: 0.5432\n",
      "Epoch 220, Loss: 0.5200\n",
      "Epoch 230, Loss: 0.5453\n",
      "Epoch 240, Loss: 0.6741\n",
      "Epoch 250, Loss: 0.5493\n",
      "Epoch 260, Loss: 0.9316\n",
      "Epoch 270, Loss: 0.5848\n",
      "Epoch 280, Loss: 0.9038\n",
      "Epoch 290, Loss: 0.5258\n",
      "Epoch 300, Loss: 0.5103\n",
      "FAE Loss: 0.510276\n",
      "Training FPCA...\n",
      "FPCA Loss: 1.043219\n",
      "\n",
      "============================================================\n",
      "Case 3 Summary:\n",
      "FAE:  1.199831 ± 0.588819\n",
      "FPCA: 1.456197 ± 0.498155\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Case 4\n",
      "============================================================\n",
      "\n",
      "--- Run 1/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 213.8812\n",
      "Epoch 20, Loss: 96.5858\n",
      "Epoch 30, Loss: 39.3472\n",
      "Epoch 40, Loss: 12.3584\n",
      "Epoch 50, Loss: 5.3607\n",
      "Epoch 60, Loss: 3.4602\n",
      "Epoch 70, Loss: 4.6409\n",
      "Epoch 80, Loss: 2.9535\n",
      "Epoch 90, Loss: 2.6043\n",
      "Epoch 100, Loss: 3.2807\n",
      "Epoch 110, Loss: 3.4480\n",
      "Epoch 120, Loss: 2.4476\n",
      "Epoch 130, Loss: 2.4916\n",
      "Epoch 140, Loss: 2.2703\n",
      "Epoch 150, Loss: 2.2353\n",
      "Epoch 160, Loss: 1.9311\n",
      "Epoch 170, Loss: 1.9195\n",
      "Epoch 180, Loss: 2.9372\n",
      "Epoch 190, Loss: 3.0084\n",
      "Epoch 200, Loss: 3.4182\n",
      "Epoch 210, Loss: 1.9426\n",
      "Epoch 220, Loss: 2.1999\n",
      "Epoch 230, Loss: 1.7526\n",
      "Epoch 240, Loss: 1.5979\n",
      "Epoch 250, Loss: 3.0080\n",
      "Epoch 260, Loss: 2.0217\n",
      "Epoch 270, Loss: 1.9665\n",
      "Epoch 280, Loss: 1.6226\n",
      "Epoch 290, Loss: 1.9078\n",
      "Epoch 300, Loss: 1.5272\n",
      "FAE Loss: 1.527247\n",
      "Training FPCA...\n",
      "FPCA Loss: 9.535774\n",
      "\n",
      "--- Run 2/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 226.5592\n",
      "Epoch 20, Loss: 142.4410\n",
      "Epoch 30, Loss: 118.2234\n",
      "Epoch 40, Loss: 96.5345\n",
      "Epoch 50, Loss: 64.2999\n",
      "Epoch 60, Loss: 18.8355\n",
      "Epoch 70, Loss: 6.1503\n",
      "Epoch 80, Loss: 4.0449\n",
      "Epoch 90, Loss: 4.3706\n",
      "Epoch 100, Loss: 3.6421\n",
      "Epoch 110, Loss: 2.8077\n",
      "Epoch 120, Loss: 2.8829\n",
      "Epoch 130, Loss: 2.7057\n",
      "Epoch 140, Loss: 2.4536\n",
      "Epoch 150, Loss: 1.9433\n",
      "Epoch 160, Loss: 2.9167\n",
      "Epoch 170, Loss: 2.2743\n",
      "Epoch 180, Loss: 1.7142\n",
      "Epoch 190, Loss: 1.8450\n",
      "Epoch 200, Loss: 1.8458\n",
      "Epoch 210, Loss: 1.4483\n",
      "Epoch 220, Loss: 2.0050\n",
      "Epoch 230, Loss: 2.0137\n",
      "Epoch 240, Loss: 1.8671\n",
      "Epoch 250, Loss: 4.2479\n",
      "Epoch 260, Loss: 2.9588\n",
      "Epoch 270, Loss: 1.4420\n",
      "Epoch 280, Loss: 1.1750\n",
      "Epoch 290, Loss: 1.1584\n",
      "Epoch 300, Loss: 1.2410\n",
      "FAE Loss: 1.241006\n",
      "Training FPCA...\n",
      "FPCA Loss: 10.824336\n",
      "\n",
      "--- Run 3/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 170.9009\n",
      "Epoch 20, Loss: 64.5532\n",
      "Epoch 30, Loss: 7.4921\n",
      "Epoch 40, Loss: 3.0564\n",
      "Epoch 50, Loss: 2.1584\n",
      "Epoch 60, Loss: 2.3363\n",
      "Epoch 70, Loss: 1.4389\n",
      "Epoch 80, Loss: 1.2527\n",
      "Epoch 90, Loss: 1.1720\n",
      "Epoch 100, Loss: 2.3844\n",
      "Epoch 110, Loss: 1.3014\n",
      "Epoch 120, Loss: 1.0798\n",
      "Epoch 130, Loss: 0.9168\n",
      "Epoch 140, Loss: 1.0788\n",
      "Epoch 150, Loss: 1.2548\n",
      "Epoch 160, Loss: 0.8437\n",
      "Epoch 170, Loss: 0.7790\n",
      "Epoch 180, Loss: 0.9550\n",
      "Epoch 190, Loss: 1.0692\n",
      "Epoch 200, Loss: 2.7912\n",
      "Epoch 210, Loss: 0.8040\n",
      "Epoch 220, Loss: 0.7160\n",
      "Epoch 230, Loss: 0.6951\n",
      "Epoch 240, Loss: 0.6852\n",
      "Epoch 250, Loss: 0.7359\n",
      "Epoch 260, Loss: 0.7318\n",
      "Epoch 270, Loss: 1.5270\n",
      "Epoch 280, Loss: 0.7548\n",
      "Epoch 290, Loss: 1.1574\n",
      "Epoch 300, Loss: 0.6647\n",
      "FAE Loss: 0.664726\n",
      "Training FPCA...\n",
      "FPCA Loss: 7.829084\n",
      "\n",
      "--- Run 4/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 199.3957\n",
      "Epoch 20, Loss: 146.2787\n",
      "Epoch 30, Loss: 137.1698\n",
      "Epoch 40, Loss: 87.6112\n",
      "Epoch 50, Loss: 35.3719\n",
      "Epoch 60, Loss: 11.6453\n",
      "Epoch 70, Loss: 7.7198\n",
      "Epoch 80, Loss: 6.2766\n",
      "Epoch 90, Loss: 5.4673\n",
      "Epoch 100, Loss: 4.0823\n",
      "Epoch 110, Loss: 5.1340\n",
      "Epoch 120, Loss: 3.6177\n",
      "Epoch 130, Loss: 3.0207\n",
      "Epoch 140, Loss: 3.3049\n",
      "Epoch 150, Loss: 2.7079\n",
      "Epoch 160, Loss: 2.3138\n",
      "Epoch 170, Loss: 2.3106\n",
      "Epoch 180, Loss: 2.7246\n",
      "Epoch 190, Loss: 2.2944\n",
      "Epoch 200, Loss: 2.1744\n",
      "Epoch 210, Loss: 2.1601\n",
      "Epoch 220, Loss: 1.7041\n",
      "Epoch 230, Loss: 1.9926\n",
      "Epoch 240, Loss: 2.2152\n",
      "Epoch 250, Loss: 1.5659\n",
      "Epoch 260, Loss: 1.5462\n",
      "Epoch 270, Loss: 1.6205\n",
      "Epoch 280, Loss: 1.9404\n",
      "Epoch 290, Loss: 2.0592\n",
      "Epoch 300, Loss: 2.6907\n",
      "FAE Loss: 2.690668\n",
      "Training FPCA...\n",
      "FPCA Loss: 10.173876\n",
      "\n",
      "--- Run 5/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 179.8769\n",
      "Epoch 20, Loss: 131.6336\n",
      "Epoch 30, Loss: 107.9214\n",
      "Epoch 40, Loss: 98.4557\n",
      "Epoch 50, Loss: 72.3359\n",
      "Epoch 60, Loss: 35.8724\n",
      "Epoch 70, Loss: 10.0728\n",
      "Epoch 80, Loss: 5.6169\n",
      "Epoch 90, Loss: 3.4946\n",
      "Epoch 100, Loss: 3.2785\n",
      "Epoch 110, Loss: 3.0376\n",
      "Epoch 120, Loss: 2.9022\n",
      "Epoch 130, Loss: 2.8064\n",
      "Epoch 140, Loss: 2.5845\n",
      "Epoch 150, Loss: 2.3476\n",
      "Epoch 160, Loss: 2.4167\n",
      "Epoch 170, Loss: 2.2546\n",
      "Epoch 180, Loss: 2.1076\n",
      "Epoch 190, Loss: 1.9372\n",
      "Epoch 200, Loss: 1.9376\n",
      "Epoch 210, Loss: 1.8281\n",
      "Epoch 220, Loss: 1.7004\n",
      "Epoch 230, Loss: 2.3315\n",
      "Epoch 240, Loss: 1.7281\n",
      "Epoch 250, Loss: 1.5947\n",
      "Epoch 260, Loss: 1.6431\n",
      "Epoch 270, Loss: 1.3991\n",
      "Epoch 280, Loss: 1.7119\n",
      "Epoch 290, Loss: 1.3540\n",
      "Epoch 300, Loss: 1.6641\n",
      "FAE Loss: 1.664065\n",
      "Training FPCA...\n",
      "FPCA Loss: 8.756247\n",
      "\n",
      "============================================================\n",
      "Case 4 Summary:\n",
      "FAE:  1.557542 ± 0.662353\n",
      "FPCA: 9.423863 ± 1.051228\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Case 5\n",
      "============================================================\n",
      "\n",
      "--- Run 1/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 276.6795\n",
      "Epoch 20, Loss: 74.1988\n",
      "Epoch 30, Loss: 51.5012\n",
      "Epoch 40, Loss: 29.9153\n",
      "Epoch 50, Loss: 25.4170\n",
      "Epoch 60, Loss: 20.3445\n",
      "Epoch 70, Loss: 16.8873\n",
      "Epoch 80, Loss: 15.0378\n",
      "Epoch 90, Loss: 13.9862\n",
      "Epoch 100, Loss: 13.3571\n",
      "Epoch 110, Loss: 11.9996\n",
      "Epoch 120, Loss: 11.1885\n",
      "Epoch 130, Loss: 11.5562\n",
      "Epoch 140, Loss: 10.8376\n",
      "Epoch 150, Loss: 10.1594\n",
      "Epoch 160, Loss: 10.0965\n",
      "Epoch 170, Loss: 10.5846\n",
      "Epoch 180, Loss: 8.8533\n",
      "Epoch 190, Loss: 8.4809\n",
      "Epoch 200, Loss: 8.0478\n",
      "Epoch 210, Loss: 7.6974\n",
      "Epoch 220, Loss: 9.1579\n",
      "Epoch 230, Loss: 6.6497\n",
      "Epoch 240, Loss: 6.7259\n",
      "Epoch 250, Loss: 10.1647\n",
      "Epoch 260, Loss: 5.5662\n",
      "Epoch 270, Loss: 4.7156\n",
      "Epoch 280, Loss: 5.2148\n",
      "Epoch 290, Loss: 3.9902\n",
      "Epoch 300, Loss: 3.5444\n",
      "FAE Loss: 3.544422\n",
      "Training FPCA...\n",
      "FPCA Loss: 15.206566\n",
      "\n",
      "--- Run 2/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 194.6131\n",
      "Epoch 20, Loss: 115.6267\n",
      "Epoch 30, Loss: 112.8425\n",
      "Epoch 40, Loss: 112.3368\n",
      "Epoch 50, Loss: 111.8967\n",
      "Epoch 60, Loss: 111.0814\n",
      "Epoch 70, Loss: 108.1824\n",
      "Epoch 80, Loss: 56.4502\n",
      "Epoch 90, Loss: 20.5653\n",
      "Epoch 100, Loss: 19.7015\n",
      "Epoch 110, Loss: 12.5632\n",
      "Epoch 120, Loss: 9.9587\n",
      "Epoch 130, Loss: 8.8875\n",
      "Epoch 140, Loss: 7.6737\n",
      "Epoch 150, Loss: 7.0553\n",
      "Epoch 160, Loss: 6.6762\n",
      "Epoch 170, Loss: 6.2890\n",
      "Epoch 180, Loss: 5.7452\n",
      "Epoch 190, Loss: 6.0420\n",
      "Epoch 200, Loss: 5.4010\n",
      "Epoch 210, Loss: 4.9848\n",
      "Epoch 220, Loss: 4.9335\n",
      "Epoch 230, Loss: 7.2258\n",
      "Epoch 240, Loss: 5.1524\n",
      "Epoch 250, Loss: 4.7692\n",
      "Epoch 260, Loss: 4.6402\n",
      "Epoch 270, Loss: 4.4250\n",
      "Epoch 280, Loss: 4.9076\n",
      "Epoch 290, Loss: 4.5952\n",
      "Epoch 300, Loss: 4.7552\n",
      "FAE Loss: 4.755183\n",
      "Training FPCA...\n",
      "FPCA Loss: 11.092092\n",
      "\n",
      "--- Run 3/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 215.3362\n",
      "Epoch 20, Loss: 111.1285\n",
      "Epoch 30, Loss: 104.9729\n",
      "Epoch 40, Loss: 105.0565\n",
      "Epoch 50, Loss: 104.5694\n",
      "Epoch 60, Loss: 103.5596\n",
      "Epoch 70, Loss: 97.3783\n",
      "Epoch 80, Loss: 56.5343\n",
      "Epoch 90, Loss: 17.0945\n",
      "Epoch 100, Loss: 12.7310\n",
      "Epoch 110, Loss: 9.5049\n",
      "Epoch 120, Loss: 8.6216\n",
      "Epoch 130, Loss: 8.4394\n",
      "Epoch 140, Loss: 6.7394\n",
      "Epoch 150, Loss: 5.6409\n",
      "Epoch 160, Loss: 4.5696\n",
      "Epoch 170, Loss: 4.8652\n",
      "Epoch 180, Loss: 2.8065\n",
      "Epoch 190, Loss: 3.1344\n",
      "Epoch 200, Loss: 1.8382\n",
      "Epoch 210, Loss: 1.3453\n",
      "Epoch 220, Loss: 1.2970\n",
      "Epoch 230, Loss: 2.5478\n",
      "Epoch 240, Loss: 1.1000\n",
      "Epoch 250, Loss: 1.0131\n",
      "Epoch 260, Loss: 1.1062\n",
      "Epoch 270, Loss: 1.1389\n",
      "Epoch 280, Loss: 0.8491\n",
      "Epoch 290, Loss: 0.9009\n",
      "Epoch 300, Loss: 2.4062\n",
      "FAE Loss: 2.406174\n",
      "Training FPCA...\n",
      "FPCA Loss: 11.554184\n",
      "\n",
      "--- Run 4/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 255.1591\n",
      "Epoch 20, Loss: 146.7540\n",
      "Epoch 30, Loss: 142.2052\n",
      "Epoch 40, Loss: 102.2824\n",
      "Epoch 50, Loss: 33.9654\n",
      "Epoch 60, Loss: 23.0577\n",
      "Epoch 70, Loss: 16.7869\n",
      "Epoch 80, Loss: 13.0518\n",
      "Epoch 90, Loss: 10.1151\n",
      "Epoch 100, Loss: 8.4818\n",
      "Epoch 110, Loss: 6.4678\n",
      "Epoch 120, Loss: 4.8911\n",
      "Epoch 130, Loss: 4.8336\n",
      "Epoch 140, Loss: 3.1111\n",
      "Epoch 150, Loss: 3.0639\n",
      "Epoch 160, Loss: 2.7008\n",
      "Epoch 170, Loss: 2.7656\n",
      "Epoch 180, Loss: 2.6731\n",
      "Epoch 190, Loss: 2.2542\n",
      "Epoch 200, Loss: 3.0052\n",
      "Epoch 210, Loss: 2.1205\n",
      "Epoch 220, Loss: 1.9760\n",
      "Epoch 230, Loss: 1.7307\n",
      "Epoch 240, Loss: 1.8822\n",
      "Epoch 250, Loss: 3.2250\n",
      "Epoch 260, Loss: 2.0400\n",
      "Epoch 270, Loss: 1.5906\n",
      "Epoch 280, Loss: 1.4013\n",
      "Epoch 290, Loss: 1.3763\n",
      "Epoch 300, Loss: 1.5570\n",
      "FAE Loss: 1.557021\n",
      "Training FPCA...\n",
      "FPCA Loss: 12.965242\n",
      "\n",
      "--- Run 5/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 208.2599\n",
      "Epoch 20, Loss: 116.8335\n",
      "Epoch 30, Loss: 93.2294\n",
      "Epoch 40, Loss: 59.5089\n",
      "Epoch 50, Loss: 51.9907\n",
      "Epoch 60, Loss: 36.9102\n",
      "Epoch 70, Loss: 18.7960\n",
      "Epoch 80, Loss: 13.1467\n",
      "Epoch 90, Loss: 10.6715\n",
      "Epoch 100, Loss: 9.3399\n",
      "Epoch 110, Loss: 8.0663\n",
      "Epoch 120, Loss: 6.9317\n",
      "Epoch 130, Loss: 6.1723\n",
      "Epoch 140, Loss: 5.4117\n",
      "Epoch 150, Loss: 4.7170\n",
      "Epoch 160, Loss: 5.8111\n",
      "Epoch 170, Loss: 3.7531\n",
      "Epoch 180, Loss: 3.4370\n",
      "Epoch 190, Loss: 3.5587\n",
      "Epoch 200, Loss: 2.6533\n",
      "Epoch 210, Loss: 2.6081\n",
      "Epoch 220, Loss: 2.2257\n",
      "Epoch 230, Loss: 1.8236\n",
      "Epoch 240, Loss: 1.7962\n",
      "Epoch 250, Loss: 1.5536\n",
      "Epoch 260, Loss: 1.4784\n",
      "Epoch 270, Loss: 1.5859\n",
      "Epoch 280, Loss: 1.2286\n",
      "Epoch 290, Loss: 1.9902\n",
      "Epoch 300, Loss: 1.1231\n",
      "FAE Loss: 1.123131\n",
      "Training FPCA...\n",
      "FPCA Loss: 11.394875\n",
      "\n",
      "============================================================\n",
      "Case 5 Summary:\n",
      "FAE:  2.677186 ± 1.327665\n",
      "FPCA: 12.442592 ± 1.524700\n",
      "============================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULTS: Reconstruction Loss Comparison (MSE)\n",
      "================================================================================\n",
      " Case            FAE_Loss            FPCA_Loss Winner\n",
      "    1 1.377205 ± 0.645409 10.007184 ± 0.450656    FAE\n",
      "    2 1.739210 ± 1.285905  1.436265 ± 0.294106   FPCA\n",
      "    3 1.199831 ± 0.588819  1.456197 ± 0.498155    FAE\n",
      "    4 1.557542 ± 0.662353  9.423863 ± 1.051228    FAE\n",
      "    5 2.677186 ± 1.327665 12.442592 ± 1.524700    FAE\n",
      "================================================================================\n",
      "\n",
      "Summary: FAE wins in 4 cases, FPCA wins in 1 cases\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "n_samples = 100\n",
    "T = 51\n",
    "delta = 0.1\n",
    "n_runs = 5\n",
    "cases = [1, 2, 3, 4, 5]\n",
    "\n",
    "# FAE parameters\n",
    "fae_params = {\n",
    "    'epochs': 300,\n",
    "    'lr': 0.005,\n",
    "    'batch_size': 32,\n",
    "    'num_basis': 20,\n",
    "    'hidden_dim': 32,\n",
    "    'hidden_dim2': 16,\n",
    "    'latent_dim': 8,\n",
    "    'degree': 3\n",
    "}\n",
    "\n",
    "# FPCA parameters\n",
    "fpca_n_components = 8 \n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'Case': [],\n",
    "    'FAE_Loss_Mean': [],\n",
    "    'FAE_Loss_Std': [],\n",
    "    'FPCA_Loss_Mean': [],\n",
    "    'FPCA_Loss_Std': []\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Comparing FAE and FPCA Reconstruction Loss\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for case in cases:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Case {case}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    fae_losses = []\n",
    "    fpca_losses = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n--- Run {run + 1}/{n_runs} ---\")\n",
    "        \n",
    "        seed = case * 100 + run\n",
    "        t_grid, X_true, Y_obs = simulate_case(case=case, n=n_samples, T=T, delta=delta, seed=seed)\n",
    "        \n",
    "        # Train FAE\n",
    "        print(\"Training FAE...\")\n",
    "        fae_model, fae_loss = train_fae(\n",
    "            data=Y_obs,\n",
    "            t_grid=t_grid,\n",
    "            **fae_params\n",
    "        )\n",
    "        fae_losses.append(fae_loss)\n",
    "        print(f\"FAE Loss: {fae_loss:.6f}\")\n",
    "        \n",
    "        # Train FPCA\n",
    "        print(\"Training FPCA...\")\n",
    "        fpca_model, fpca_scores, fpca_recon, fpca_loss = train_fpca_skfda(\n",
    "            data=Y_obs,\n",
    "            t_grid=t_grid,\n",
    "            n_components=fpca_n_components\n",
    "        )\n",
    "        fpca_losses.append(fpca_loss)\n",
    "        print(f\"FPCA Loss: {fpca_loss:.6f}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    fae_mean = np.mean(fae_losses)\n",
    "    fae_std = np.std(fae_losses)\n",
    "    fpca_mean = np.mean(fpca_losses)\n",
    "    fpca_std = np.std(fpca_losses)\n",
    "    \n",
    "    results['Case'].append(case)\n",
    "    results['FAE_Loss_Mean'].append(fae_mean)\n",
    "    results['FAE_Loss_Std'].append(fae_std)\n",
    "    results['FPCA_Loss_Mean'].append(fpca_mean)\n",
    "    results['FPCA_Loss_Std'].append(fpca_std)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Case {case} Summary:\")\n",
    "    print(f\"FAE:  {fae_mean:.6f} ± {fae_std:.6f}\")\n",
    "    print(f\"FPCA: {fpca_mean:.6f} ± {fpca_std:.6f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Create and display results table\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS: Reconstruction Loss Comparison (MSE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['FAE_Loss'] = df['FAE_Loss_Mean'].apply(lambda x: f\"{x:.6f}\") + \" ± \" + df['FAE_Loss_Std'].apply(lambda x: f\"{x:.6f}\")\n",
    "df['FPCA_Loss'] = df['FPCA_Loss_Mean'].apply(lambda x: f\"{x:.6f}\") + \" ± \" + df['FPCA_Loss_Std'].apply(lambda x: f\"{x:.6f}\")\n",
    "df['Winner'] = df.apply(lambda row: 'FAE' if row['FAE_Loss_Mean'] < row['FPCA_Loss_Mean'] else 'FPCA', axis=1)\n",
    "\n",
    "# Display the formatted table\n",
    "result_table = df[['Case', 'FAE_Loss', 'FPCA_Loss', 'Winner']]\n",
    "print(result_table.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Additional statistics\n",
    "fae_wins = (df['Winner'] == 'FAE').sum()\n",
    "fpca_wins = (df['Winner'] == 'FPCA').sum()\n",
    "print(f\"\\nSummary: FAE wins in {fae_wins} cases, FPCA wins in {fpca_wins} cases\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e7d88",
   "metadata": {},
   "source": [
    "#### Data Loader from UCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65776b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering evaluation functions loaded.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def clustering_accuracy(y_true, y_pred):\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    y_pred = y_pred.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    accuracy = w[row_ind, col_ind].sum() / y_pred.size\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_clustering(embeddings, true_labels, n_clusters, n_runs=10):\n",
    "\n",
    "    acc_list = []\n",
    "    ari_list = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=None)\n",
    "        pred_labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        acc = clustering_accuracy(true_labels, pred_labels)\n",
    "        ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "        \n",
    "        acc_list.append(acc)\n",
    "        ari_list.append(ari)\n",
    "    \n",
    "    return max(acc_list), max(ari_list), np.mean(acc_list), np.mean(ari_list)\n",
    "\n",
    "print(\"Clustering evaluation functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c2e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BME...\n",
      "  Dataset shape: (180, 128)\n",
      "  Number of samples: 180\n",
      "  Number of time points: 128\n",
      "  Number of classes: 3\n",
      "Loading DiatomSizeReduction...\n",
      "  Dataset shape: (322, 345)\n",
      "  Number of samples: 322\n",
      "  Number of time points: 345\n",
      "  Number of classes: 4\n",
      "Loading Plane...\n",
      "  Dataset shape: (210, 144)\n",
      "  Number of samples: 210\n",
      "  Number of time points: 144\n",
      "  Number of classes: 7\n",
      "Loading Fungi...\n",
      "  Dataset shape: (204, 201)\n",
      "  Number of samples: 204\n",
      "  Number of time points: 201\n",
      "  Number of classes: 18\n",
      "\n",
      "Successfully loaded 4 datasets\n"
     ]
    }
   ],
   "source": [
    "from aeon.datasets import load_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_ucr_dataset(dataset_name):\n",
    "\n",
    "    print(f\"Loading {dataset_name}...\")\n",
    "    \n",
    "    # Load train and test data\n",
    "    X_train, y_train = load_classification(dataset_name, split=\"train\")\n",
    "    X_test, y_test = load_classification(dataset_name, split=\"test\")\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    if hasattr(X_train, 'to_numpy'):\n",
    "        X_train = X_train.to_numpy()\n",
    "    if hasattr(X_test, 'to_numpy'):\n",
    "        X_test = X_test.to_numpy()\n",
    "    \n",
    "    # Squeeze if multivariate (we'll use first dimension for univariate)\n",
    "    if len(X_train.shape) == 3:\n",
    "        X_train = X_train[:, 0, :]  # Take first dimension\n",
    "        X_test = X_test[:, 0, :]\n",
    "    \n",
    "    # Merge train and test\n",
    "    data = np.vstack([X_train, X_test])\n",
    "    labels = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    # Create time grid\n",
    "    n_timepoints = data.shape[1]\n",
    "    t_grid = np.linspace(0.0, 1.0, n_timepoints)\n",
    "    \n",
    "    print(f\"  Dataset shape: {data.shape}\")\n",
    "    print(f\"  Number of samples: {len(data)}\")\n",
    "    print(f\"  Number of time points: {n_timepoints}\")\n",
    "    print(f\"  Number of classes: {len(np.unique(labels))}\")\n",
    "    \n",
    "    return data, labels, t_grid\n",
    "\n",
    "\n",
    "# Load UCR datasets\n",
    "ucr_datasets = {\n",
    "    'BME': 'BME',\n",
    "    'DiatomSizeReduction': 'DiatomSizeReduction',\n",
    "    'Plane': 'Plane', \n",
    "    'Fungi': 'Fungi',\n",
    "}\n",
    "\n",
    "# Store all datasets\n",
    "datasets_dict = {}\n",
    "for short_name, full_name in ucr_datasets.items():\n",
    "    try:\n",
    "        data, labels, t_grid = load_ucr_dataset(full_name)\n",
    "        datasets_dict[short_name] = {\n",
    "            'data': data,\n",
    "            'labels': labels,\n",
    "            't_grid': t_grid\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {full_name}: {e}\")\n",
    "        print(f\"Skipping {short_name}...\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(datasets_dict)} datasets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b85cd7",
   "metadata": {},
   "source": [
    "### Comparison of cluster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e387fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Comparing FAE and FPCA Clustering Performance on UCR Datasets\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Dataset: BME\n",
      "====================================================================================================\n",
      "Shape: (180, 128)\n",
      "Classes: 3\n",
      "\n",
      "--- Model Training Run 1/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 15.3288\n",
      "Epoch 20, Loss: 7.0923\n",
      "Epoch 30, Loss: 7.0088\n",
      "Epoch 40, Loss: 6.9179\n",
      "Epoch 50, Loss: 6.7722\n",
      "Epoch 60, Loss: 6.5297\n",
      "Epoch 70, Loss: 6.3861\n",
      "Epoch 80, Loss: 6.1109\n",
      "Epoch 90, Loss: 5.4392\n",
      "Epoch 100, Loss: 5.1540\n",
      "Epoch 110, Loss: 5.0559\n",
      "Epoch 120, Loss: 4.9707\n",
      "Epoch 130, Loss: 4.8918\n",
      "Epoch 140, Loss: 4.8174\n",
      "Epoch 150, Loss: 4.7688\n",
      "Epoch 160, Loss: 4.7083\n",
      "Epoch 170, Loss: 4.6164\n",
      "Epoch 180, Loss: 4.5571\n",
      "Epoch 190, Loss: 4.5429\n",
      "Epoch 200, Loss: 4.4926\n",
      "Epoch 210, Loss: 4.4461\n",
      "Epoch 220, Loss: 4.4216\n",
      "Epoch 230, Loss: 4.4074\n",
      "Epoch 240, Loss: 4.3931\n",
      "Epoch 250, Loss: 4.3890\n",
      "Epoch 260, Loss: 4.3417\n",
      "Epoch 270, Loss: 4.3933\n",
      "Epoch 280, Loss: 4.3099\n",
      "Epoch 290, Loss: 4.3485\n",
      "Epoch 300, Loss: 4.2836\n",
      "Epoch 310, Loss: 4.3081\n",
      "Epoch 320, Loss: 4.3041\n",
      "Epoch 330, Loss: 4.2483\n",
      "Epoch 340, Loss: 4.2167\n",
      "Epoch 350, Loss: 4.3016\n",
      "Epoch 360, Loss: 4.2838\n",
      "Epoch 370, Loss: 4.1115\n",
      "Epoch 380, Loss: 3.9950\n",
      "Epoch 390, Loss: 4.0111\n",
      "Epoch 400, Loss: 3.7822\n",
      "Epoch 410, Loss: 3.5376\n",
      "Epoch 420, Loss: 3.1673\n",
      "Epoch 430, Loss: 3.1015\n",
      "Epoch 440, Loss: 2.9164\n",
      "Epoch 450, Loss: 2.8341\n",
      "Epoch 460, Loss: 2.7895\n",
      "Epoch 470, Loss: 2.7219\n",
      "Epoch 480, Loss: 2.7052\n",
      "Epoch 490, Loss: 2.6874\n",
      "Epoch 500, Loss: 2.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAE - ACC: 0.5667, ARI: 0.1260\n",
      "Training FPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPCA - ACC: 0.4556, ARI: 0.1229\n",
      "\n",
      "--- Model Training Run 2/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 15.8355\n",
      "Epoch 20, Loss: 7.2119\n",
      "Epoch 30, Loss: 7.1158\n",
      "Epoch 40, Loss: 7.0777\n",
      "Epoch 50, Loss: 7.0405\n",
      "Epoch 60, Loss: 7.0372\n",
      "Epoch 70, Loss: 7.0115\n",
      "Epoch 80, Loss: 6.9775\n",
      "Epoch 90, Loss: 6.9365\n",
      "Epoch 100, Loss: 6.9193\n",
      "Epoch 110, Loss: 6.8746\n",
      "Epoch 120, Loss: 6.7956\n",
      "Epoch 130, Loss: 5.7175\n",
      "Epoch 140, Loss: 5.0554\n",
      "Epoch 150, Loss: 4.8538\n",
      "Epoch 160, Loss: 4.7437\n",
      "Epoch 170, Loss: 4.6503\n",
      "Epoch 180, Loss: 4.5868\n",
      "Epoch 190, Loss: 4.5458\n",
      "Epoch 200, Loss: 4.5185\n",
      "Epoch 210, Loss: 4.4213\n",
      "Epoch 220, Loss: 4.3252\n",
      "Epoch 230, Loss: 4.2135\n",
      "Epoch 240, Loss: 4.1573\n",
      "Epoch 250, Loss: 4.0537\n",
      "Epoch 260, Loss: 4.0225\n",
      "Epoch 270, Loss: 3.9386\n",
      "Epoch 280, Loss: 3.9330\n",
      "Epoch 290, Loss: 3.8712\n",
      "Epoch 300, Loss: 3.8222\n",
      "Epoch 310, Loss: 3.7931\n",
      "Epoch 320, Loss: 3.5547\n",
      "Epoch 330, Loss: 3.4387\n",
      "Epoch 340, Loss: 3.3780\n",
      "Epoch 350, Loss: 3.2998\n",
      "Epoch 360, Loss: 3.2311\n",
      "Epoch 370, Loss: 3.1573\n",
      "Epoch 380, Loss: 3.1034\n",
      "Epoch 390, Loss: 3.0409\n",
      "Epoch 400, Loss: 2.9748\n",
      "Epoch 410, Loss: 2.8858\n",
      "Epoch 420, Loss: 2.7882\n",
      "Epoch 430, Loss: 2.7197\n",
      "Epoch 440, Loss: 2.7062\n",
      "Epoch 450, Loss: 2.6217\n",
      "Epoch 460, Loss: 2.5319\n",
      "Epoch 470, Loss: 2.3802\n",
      "Epoch 480, Loss: 2.2397\n",
      "Epoch 490, Loss: 2.2035\n",
      "Epoch 500, Loss: 2.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAE - ACC: 0.4778, ARI: 0.1287\n",
      "Training FPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPCA - ACC: 0.4556, ARI: 0.1265\n",
      "\n",
      "--- Model Training Run 3/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 19.4738\n",
      "Epoch 20, Loss: 7.4567\n",
      "Epoch 30, Loss: 7.1391\n",
      "Epoch 40, Loss: 7.0652\n",
      "Epoch 50, Loss: 7.0074\n",
      "Epoch 60, Loss: 6.9651\n",
      "Epoch 70, Loss: 6.9160\n",
      "Epoch 80, Loss: 6.7949\n",
      "Epoch 90, Loss: 6.6295\n",
      "Epoch 100, Loss: 6.4981\n",
      "Epoch 110, Loss: 6.4884\n",
      "Epoch 120, Loss: 6.4274\n",
      "Epoch 130, Loss: 6.4093\n",
      "Epoch 140, Loss: 6.3598\n",
      "Epoch 150, Loss: 6.3223\n",
      "Epoch 160, Loss: 6.3123\n",
      "Epoch 170, Loss: 6.2456\n",
      "Epoch 180, Loss: 6.3290\n",
      "Epoch 190, Loss: 6.1277\n",
      "Epoch 200, Loss: 6.0425\n",
      "Epoch 210, Loss: 6.0198\n",
      "Epoch 220, Loss: 5.9356\n",
      "Epoch 230, Loss: 5.5960\n",
      "Epoch 240, Loss: 4.5787\n",
      "Epoch 250, Loss: 4.3242\n",
      "Epoch 260, Loss: 4.1172\n",
      "Epoch 270, Loss: 3.9980\n",
      "Epoch 280, Loss: 3.8971\n",
      "Epoch 290, Loss: 3.8431\n",
      "Epoch 300, Loss: 3.8152\n",
      "Epoch 310, Loss: 3.7727\n",
      "Epoch 320, Loss: 3.7027\n",
      "Epoch 330, Loss: 3.6752\n",
      "Epoch 340, Loss: 3.6392\n",
      "Epoch 350, Loss: 3.6423\n",
      "Epoch 360, Loss: 3.5391\n",
      "Epoch 370, Loss: 3.3991\n",
      "Epoch 380, Loss: 3.0918\n",
      "Epoch 390, Loss: 3.0127\n",
      "Epoch 400, Loss: 2.9158\n",
      "Epoch 410, Loss: 2.8967\n",
      "Epoch 420, Loss: 2.8627\n",
      "Epoch 430, Loss: 2.7787\n",
      "Epoch 440, Loss: 2.6246\n",
      "Epoch 450, Loss: 2.5150\n",
      "Epoch 460, Loss: 2.4538\n",
      "Epoch 470, Loss: 2.4519\n",
      "Epoch 480, Loss: 2.4458\n",
      "Epoch 490, Loss: 2.3641\n",
      "Epoch 500, Loss: 2.4020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAE - ACC: 0.4833, ARI: 0.1569\n",
      "Training FPCA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "d:\\Software\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPCA - ACC: 0.4556, ARI: 0.1229\n",
      "\n",
      "--- Model Training Run 4/5 ---\n",
      "Training FAE...\n",
      "\n",
      "--- Training FAE ---\n",
      "Epoch 10, Loss: 19.3375\n",
      "Epoch 20, Loss: 7.0846\n",
      "Epoch 30, Loss: 7.0062\n",
      "Epoch 40, Loss: 6.9261\n",
      "Epoch 50, Loss: 6.8382\n",
      "Epoch 60, Loss: 6.7117\n",
      "Epoch 70, Loss: 6.5398\n",
      "Epoch 80, Loss: 6.4230\n",
      "Epoch 90, Loss: 6.2836\n",
      "Epoch 100, Loss: 5.6850\n",
      "Epoch 110, Loss: 5.3473\n",
      "Epoch 120, Loss: 5.1924\n",
      "Epoch 130, Loss: 5.0961\n",
      "Epoch 140, Loss: 5.0328\n",
      "Epoch 150, Loss: 4.8846\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# FAE parameters for UCR datasets\n",
    "fae_params = {\n",
    "    'epochs': 500,\n",
    "    'lr': 0.005,\n",
    "    'batch_size': 32,\n",
    "    'num_basis': 20,\n",
    "    'hidden_dim': 32,\n",
    "    'hidden_dim2': 16,\n",
    "    'latent_dim': 8,\n",
    "    'degree': 3\n",
    "}\n",
    "\n",
    "# FPCA parameters\n",
    "fpca_n_components = 8\n",
    "n_model_runs = 5  \n",
    "n_kmeans_runs = 10  \n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'Dataset': [],\n",
    "    'N_Samples': [],\n",
    "    'N_Classes': [],\n",
    "    'N_Timepoints': [],\n",
    "    'FAE_ACC_Mean': [],\n",
    "    'FAE_ACC_Std': [],\n",
    "    'FAE_ARI_Mean': [],\n",
    "    'FAE_ARI_Std': [],\n",
    "    'FPCA_ACC_Mean': [],\n",
    "    'FPCA_ACC_Std': [],\n",
    "    'FPCA_ARI_Mean': [],\n",
    "    'FPCA_ARI_Std': []\n",
    "}\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Comparing FAE and FPCA Clustering Performance on UCR Datasets\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for dataset_name in datasets_dict.keys():\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Get dataset\n",
    "    data = datasets_dict[dataset_name]['data']\n",
    "    labels = datasets_dict[dataset_name]['labels']\n",
    "    t_grid = datasets_dict[dataset_name]['t_grid']\n",
    "    \n",
    "    # Convert labels to integers starting from 0\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "    labels_int = np.array([label_mapping[label] for label in labels])\n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Classes: {n_clusters}\")\n",
    "    \n",
    "    fae_acc_list = []\n",
    "    fae_ari_list = []\n",
    "    fpca_acc_list = []\n",
    "    fpca_ari_list = []\n",
    "    \n",
    "    for run in range(n_model_runs):\n",
    "        print(f\"\\n--- Model Training Run {run + 1}/{n_model_runs} ---\")\n",
    "        \n",
    "        # Train FAE and extract latent representations\n",
    "        print(\"Training FAE...\")\n",
    "        fae_model, fae_loss = train_fae(\n",
    "            data=data,\n",
    "            t_grid=t_grid,\n",
    "            **fae_params\n",
    "        )\n",
    "        \n",
    "        # Get FAE latent representations\n",
    "        fae_model.eval()\n",
    "        with torch.no_grad():\n",
    "            data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "            _, fae_latent = fae_model(data_tensor)\n",
    "            fae_latent_np = fae_latent.cpu().numpy()\n",
    "        \n",
    "        # Evaluate FAE clustering\n",
    "        fae_best_acc, fae_best_ari, fae_mean_acc, fae_mean_ari = evaluate_clustering(\n",
    "            fae_latent_np, labels_int, n_clusters, n_runs=n_kmeans_runs\n",
    "        )\n",
    "        fae_acc_list.append(fae_best_acc)\n",
    "        fae_ari_list.append(fae_best_ari)\n",
    "        print(f\"FAE - ACC: {fae_best_acc:.4f}, ARI: {fae_best_ari:.4f}\")\n",
    "        \n",
    "        # Train FPCA and extract scores\n",
    "        print(\"Training FPCA...\")\n",
    "        fpca_model, fpca_scores, fpca_recon, fpca_loss = train_fpca_skfda(\n",
    "            data=data,\n",
    "            t_grid=t_grid,\n",
    "            n_components=fpca_n_components\n",
    "        )\n",
    "        \n",
    "        # Evaluate FPCA clustering\n",
    "        fpca_best_acc, fpca_best_ari, fpca_mean_acc, fpca_mean_ari = evaluate_clustering(\n",
    "            fpca_scores, labels_int, n_clusters, n_runs=n_kmeans_runs\n",
    "        )\n",
    "        fpca_acc_list.append(fpca_best_acc)\n",
    "        fpca_ari_list.append(fpca_best_ari)\n",
    "        print(f\"FPCA - ACC: {fpca_best_acc:.4f}, ARI: {fpca_best_ari:.4f}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    fae_acc_mean = np.mean(fae_acc_list)\n",
    "    fae_acc_std = np.std(fae_acc_list)\n",
    "    fae_ari_mean = np.mean(fae_ari_list)\n",
    "    fae_ari_std = np.std(fae_ari_list)\n",
    "    \n",
    "    fpca_acc_mean = np.mean(fpca_acc_list)\n",
    "    fpca_acc_std = np.std(fpca_acc_list)\n",
    "    fpca_ari_mean = np.mean(fpca_ari_list)\n",
    "    fpca_ari_std = np.std(fpca_ari_list)\n",
    "    \n",
    "    results['Dataset'].append(dataset_name)\n",
    "    results['N_Samples'].append(data.shape[0])\n",
    "    results['N_Classes'].append(n_clusters)\n",
    "    results['N_Timepoints'].append(data.shape[1])\n",
    "    results['FAE_ACC_Mean'].append(fae_acc_mean)\n",
    "    results['FAE_ACC_Std'].append(fae_acc_std)\n",
    "    results['FAE_ARI_Mean'].append(fae_ari_mean)\n",
    "    results['FAE_ARI_Std'].append(fae_ari_std)\n",
    "    results['FPCA_ACC_Mean'].append(fpca_acc_mean)\n",
    "    results['FPCA_ACC_Std'].append(fpca_acc_std)\n",
    "    results['FPCA_ARI_Mean'].append(fpca_ari_mean)\n",
    "    results['FPCA_ARI_Std'].append(fpca_ari_std)\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"{dataset_name} Summary:\")\n",
    "    print(f\"FAE  - ACC: {fae_acc_mean:.4f} ± {fae_acc_std:.4f}, ARI: {fae_ari_mean:.4f} ± {fae_ari_std:.4f}\")\n",
    "    print(f\"FPCA - ACC: {fpca_acc_mean:.4f} ± {fpca_acc_std:.4f}, ARI: {fpca_ari_mean:.4f} ± {fpca_ari_std:.4f}\")\n",
    "    print(f\"Winner (ACC): {'FAE' if fae_acc_mean > fpca_acc_mean else 'FPCA'}\")\n",
    "    print(f\"Winner (ARI): {'FAE' if fae_ari_mean > fpca_ari_mean else 'FPCA'}\")\n",
    "    print(f\"{'='*100}\")\n",
    "\n",
    "# Create and display results table\n",
    "print(\"\\n\\n\" + \"=\" * 120)\n",
    "print(\"FINAL RESULTS: UCR Datasets Clustering Performance Comparison\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df['FAE_ACC'] = df['FAE_ACC_Mean'].apply(lambda x: f\"{x:.4f}\") + \" ± \" + df['FAE_ACC_Std'].apply(lambda x: f\"{x:.4f}\")\n",
    "df['FAE_ARI'] = df['FAE_ARI_Mean'].apply(lambda x: f\"{x:.4f}\") + \" ± \" + df['FAE_ARI_Std'].apply(lambda x: f\"{x:.4f}\")\n",
    "df['FPCA_ACC'] = df['FPCA_ACC_Mean'].apply(lambda x: f\"{x:.4f}\") + \" ± \" + df['FPCA_ACC_Std'].apply(lambda x: f\"{x:.4f}\")\n",
    "df['FPCA_ARI'] = df['FPCA_ARI_Mean'].apply(lambda x: f\"{x:.4f}\") + \" ± \" + df['FPCA_ARI_Std'].apply(lambda x: f\"{x:.4f}\")\n",
    "df['Winner_ACC'] = df.apply(lambda row: 'FAE' if row['FAE_ACC_Mean'] > row['FPCA_ACC_Mean'] else 'FPCA', axis=1)\n",
    "df['Winner_ARI'] = df.apply(lambda row: 'FAE' if row['FAE_ARI_Mean'] > row['FPCA_ARI_Mean'] else 'FPCA', axis=1)\n",
    "\n",
    "# Display the formatted table\n",
    "result_table = df[['Dataset', 'N_Samples', 'N_Classes', 'FAE_ACC', 'FPCA_ACC', 'Winner_ACC', 'FAE_ARI', 'FPCA_ARI', 'Winner_ARI']]\n",
    "print(result_table.to_string(index=False))\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Additional statistics\n",
    "fae_acc_wins = (df['Winner_ACC'] == 'FAE').sum()\n",
    "fpca_acc_wins = (df['Winner_ACC'] == 'FPCA').sum()\n",
    "fae_ari_wins = (df['Winner_ARI'] == 'FAE').sum()\n",
    "fpca_ari_wins = (df['Winner_ARI'] == 'FPCA').sum()\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  ACC: FAE wins in {fae_acc_wins} datasets, FPCA wins in {fpca_acc_wins} datasets\")\n",
    "print(f\"  ARI: FAE wins in {fae_ari_wins} datasets, FPCA wins in {fpca_ari_wins} datasets\")\n",
    "print(\"=\" * 120)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
